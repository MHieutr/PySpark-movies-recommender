{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommendation_system_pyspark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM7hjmb1ylnQ"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "# upload movies.csv ratings.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark \n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "metadata": {
        "id": "BDOGhAD9X1zy",
        "outputId": "dace70f7-acfa-4a00-e717-1974e1bc0787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=f3475bddb0c16621c99f95616e86b73be01faa365ee9ba1229169fb109c4506f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n",
            "openjdk-8-jdk-headless is already the newest version (8u372-ga~us1-0ubuntu1~20.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnEHi2mNyjzE"
      },
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmDJLb3iyn6t"
      },
      "source": [
        "import pandas as pd\n",
        "from pyspark.ml.feature import VectorAssembler, HashingTF, IDF, Normalizer, StopWordsRemover\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.functions import col, isnan, when, trim\n",
        "\n",
        "import pyspark.sql.functions as psf\n",
        "from pyspark.sql.types import DoubleType"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6jvRY1Nys8F"
      },
      "source": [
        "movies = spark.read.csv('movies.csv',inferSchema=True, header =True)\n",
        "ratings = spark.read.csv('ratings.csv',inferSchema=True, header =True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrDjwXsY9GgS"
      },
      "source": [
        "**Build up the content-based filtering algorithm with pairwise approach in TF-IDF vector space**\n",
        "\n",
        "The approach is based on the solution here:\n",
        "\n",
        "https://stackoverflow.com/questions/46758768/calculating-the-cosine-similarity-between-all-the-rows-of-a-dataframe-in-pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2GHdf-o9FUW"
      },
      "source": [
        "df = movies.select(\"movieId\", \"genres\").withColumn(\"genres\", psf.split( psf.lower(movies.genres), '\\|') )\n",
        "remover = StopWordsRemover(inputCol=\"genres\", outputCol=\"filtered\")\n",
        "df = remover.transform(df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfGRqYq2XJSE"
      },
      "source": [
        "Compute TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucLgofFsJr-g"
      },
      "source": [
        "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"tf\")\n",
        "tf = hashingTF.transform(df)\n",
        "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\").fit(tf)\n",
        "tfidf = idf.transform(tf)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l5rsk6dXKBO"
      },
      "source": [
        "Compute L2 norm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufh8tT9XJ1_Z"
      },
      "source": [
        "normalizer = Normalizer(inputCol=\"tfidf\", outputCol=\"norm\")\n",
        "data = normalizer.transform(tfidf)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rxeML_nXLpL"
      },
      "source": [
        "Compute matrix product (cos_similarity):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdPcMqITJ8iT"
      },
      "source": [
        "dot_udf = psf.udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
        "\n",
        "cos_similarity = data.alias(\"i\").join(data.alias(\"j\"), psf.col(\"i.movieId\") < psf.col(\"j.movieId\"))\\\n",
        "                     .select(\n",
        "                         psf.col(\"i.movieId\").alias(\"i\"), \n",
        "                         psf.col(\"j.movieId\").alias(\"j\"), \n",
        "                         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\")).sort(\"i\", \"j\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxAW01RU0Z99"
      },
      "source": [
        "**Build up the Alternating Least Square (ALS) matrix factorization model in collaborative filtering algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZxzorlY-0X0"
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = ratings.randomSplit([0.8,0.2])\n",
        "\n",
        "als = ALS(userCol='userId',itemCol='movieId',ratingCol='rating',coldStartStrategy=\"drop\")\n",
        "\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(als.regParam, [1, 0.1, 0.01]) \\\n",
        "    .addGrid(als.rank, [10, 20]) \\\n",
        "    .build()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUfr0t18_QVg"
      },
      "source": [
        "crossval = CrossValidator(estimator=als,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"mae\"),\n",
        "                          numFolds=3)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxFsL1qG_Q7b"
      },
      "source": [
        "cvModel = crossval.fit(train_data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvBrF78fKSbB"
      },
      "source": [
        "best_rank = cvModel.bestModel._java_obj.parent().getRank()\n",
        "best_regParam = cvModel.bestModel._java_obj.parent().getRegParam()\n",
        "best_model_params = {'rank': best_rank, 'regParam': best_regParam}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9aAvIVM_UdU"
      },
      "source": [
        "pred = cvModel.transform(test_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL5e7nZBa5VU"
      },
      "source": [
        "def get_movieId( movie_name, movies_data ):\n",
        "    \"\"\"\n",
        "    return the movieId which is corresponding to the movie name\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    movie_name: string, the name of the movie w/ or w/o the year\n",
        "\n",
        "    movies_data: spark Dataframe, movies data with columns 'movieId','title'\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    the movieId\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    movieIds = []\n",
        "    for movie in movie_name:\n",
        "      Ids = movies_data.filter(movies_data.title.like('%{}%'.format(movie)) ).select('movieId').collect()\n",
        "      movieIds = list(set(movieIds + [ row.movieId for row in Ids ]  ))\n",
        "    return movieIds"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkaHOQ6H9Ink"
      },
      "source": [
        "**Make movie recommendation (item-based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IwN-XalXaaB"
      },
      "source": [
        "def make_recommendation_item_based(similarity_matrix, ratings_data, movies_data,\n",
        "                        fav_movie, n_recommendations, userId=-99 ):\n",
        "    \"\"\"\n",
        "    Make top n movie recommendations. Currently, the movies the old user have watched are not excluded in the recommendation list yet.\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    similarity_matrix: spark Dataframe, the similarity matrix with columns 'i','j','dot'\n",
        "\n",
        "    ratings_data: spark Dataframe, ratings data with columns 'userId', 'movieId', 'rating' \n",
        "\n",
        "    movies_data: spark Dataframe, movies data with columns 'movieId','title'\n",
        "\n",
        "    fav_movie: str, name of user input movie\n",
        "\n",
        "    n_recommendations: int, top n recommendations\n",
        "\n",
        "    userId: int optional (default=-99), the user Id\n",
        "            if userId = -99, the new user will be created\n",
        "            if userId = -1, the latest inserted user is chosen\n",
        "\n",
        "    \"\"\"  \n",
        "  \n",
        "    movieIds = get_movieId(fav_movie, movies_data )\n",
        "\n",
        "    if (userId == -99):\n",
        "      userId = ratings_data.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
        "    elif (userId == -1):\n",
        "      userId = ratings_data.agg({\"userId\": \"max\"}).collect()[0][0]\n",
        "\n",
        "  \n",
        "    df_similar_movieIds = similarity_matrix.filter( similarity_matrix.i.isin(movieIds) ).select('i','j','dot') \n",
        "  \n",
        "    df_similar_movieIds = df_similar_movieIds.filter( ~similarity_matrix.j.isin(movieIds) ).select('i','j','dot')\n",
        "\n",
        "    df_similar_movieIds = df_similar_movieIds.groupBy('j').agg( {'dot':'max'} ).select(col('j').alias('movieId'), col('max(dot)').alias('dot_max'))\n",
        " \n",
        "    topn_predictions = df_similar_movieIds.orderBy('dot_max', ascending=False).limit(10)\n",
        "\n",
        "    Ids = topn_predictions.select('movieId').collect()\n",
        "    Ids = [ row.movieId for row in Ids ]\n",
        "    topn_movies = movies_data.filter( movies_data.movieId.isin(Ids) ).select( 'title' )\n",
        "\n",
        "#    The following line is better, but it will produce error message...\n",
        "#    movieId#14 are ambiguous. It's probably because you joined several Datasets together, and some of these......\n",
        "#    topn_movies = movies_data.join( topn_predictions, topn_predictions.movieId == movies_data.movieId ).orderBy( 'dot_max', ascending=False ).select( 'title' )\n",
        "\n",
        "    return [row.title for row in topn_movies.collect()]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlUfZcys9DYe"
      },
      "source": [
        "**Make movie recommendation (user-based)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NprIVyJAXk91"
      },
      "source": [
        "def make_recommendation_user_based(best_model_params, ratings_data, movies_data,\n",
        "                        fav_movie, n_recommendations, userId=-99 ):\n",
        "    \"\"\"\n",
        "    make top n movie recommendations\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    best_model_params: dict, the best parameters of the model from the CrossValidator\n",
        "\n",
        "    ratings_data: spark Dataframe, ratings data with columns 'userId', 'movieId', 'rating' \n",
        "\n",
        "    movies_data: spark Dataframe, movies data with columns 'movieId','title'\n",
        "\n",
        "    fav_movie: str, name of user input movie\n",
        "\n",
        "    n_recommendations: int, top n recommendations\n",
        "\n",
        "    userId: int optional (default=-99), the user Id\n",
        "            if userId = -99, the new user will be created\n",
        "            if userId = -1, the latest inserted user is chosen\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    movieIds = get_movieId(fav_movie, movies_data )\n",
        "\n",
        "    if (userId == -99):\n",
        "      userId = ratings_data.agg({\"userId\": \"max\"}).collect()[0][0] + 1\n",
        "    elif (userId == -1):\n",
        "      userId = ratings_data.agg({\"userId\": \"max\"}).collect()[0][0]\n",
        "\n",
        "    max_rating = ratings_data.agg({\"rating\": \"max\"}).collect()[0][0]\n",
        "\n",
        "    # build up the train data, which is the original data + the new inserted data.\n",
        "    # We assume that the inserted favorate movie has the highest rating.\n",
        "    train_data = ratings_data\n",
        "    for movieId in movieIds:\n",
        "      new_rows = spark.createDataFrame([(userId,movieId,max_rating,0)], ['userId', 'movieId', 'rating', 'timestamp'])\n",
        "      train_data = ratings_data.union(new_rows)\n",
        "\n",
        "    # train best ALS\n",
        "    als = ALS(userCol='userId',itemCol='movieId',ratingCol='rating', \\\n",
        "              rank=best_model_params.get('rank'), \\\n",
        "              regParam=best_model_params.get('regParam'))\n",
        "\n",
        "    model = als.fit( train_data )\n",
        "    df_newuser = movies_data.filter(~movies_data.movieId.isin(movieIds)).select('movieId').withColumn(\"userId\", lit(userId))\n",
        "\n",
        "    predictions = model.transform(df_newuser)\n",
        "\n",
        "    def to_null(c):\n",
        "      return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
        "    \n",
        "    predictions = predictions.select([to_null(c).alias(c) for c in predictions.columns]).na.drop()\n",
        "\n",
        "    topn_predictions = predictions.orderBy('prediction', ascending=False).limit(n_recommendations)\n",
        "    topn_ids = topn_predictions.select('userId')\n",
        "    topn_movies = movies_data.join( topn_predictions, topn_predictions.movieId == movies_data.movieId ).orderBy( 'prediction', ascending=False ).select( 'title' )\n",
        "\n",
        "    return [row.title for row in topn_movies.collect()]\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHE6EVb54Esq",
        "outputId": "a1004c5e-5256-40ad-b61d-78f492487951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#my_favorite_movies = ['Iron Man']\n",
        "my_favorite_movies = ['Genius Party']\n",
        "# get recommends\n",
        "n_recommendations = 10\n",
        "recommends_item_based = make_recommendation_item_based(similarity_matrix = cos_similarity, ratings_data = ratings, movies_data = movies,\n",
        "                        fav_movie = my_favorite_movies, n_recommendations = n_recommendations )\n",
        "\n",
        "print(\"--------------Search based on similarity between movies--------------------------------------\")\n",
        "print('The users like' , my_favorite_movies , 'also like:')\n",
        "for i, title in enumerate(recommends_item_based):\n",
        "    print(i+1, title)\n",
        "if( len(recommends_item_based) < n_recommendations ):\n",
        "  print(\"Sadly, we couldn't offer so many recommendations :(\")\n",
        "\n",
        "recommends_user_based = make_recommendation_user_based(best_model_params = best_model_params, ratings_data = ratings, movies_data = movies,\n",
        "                        fav_movie = my_favorite_movies, n_recommendations = n_recommendations )\n",
        "\n",
        "print(\"--------------Search based on similarity between user's preference--------------------------------------\")\n",
        "print('The users like' , my_favorite_movies , 'also like:')\n",
        "for i, title in enumerate(recommends_user_based):\n",
        "    print(i+1, title)\n",
        "if( len(recommends_user_based) < n_recommendations ):\n",
        "  print(\"Sadly, we couldn't offer so many recommendations :(\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------Search based on similarity between movies--------------------------------------\n",
            "The users like ['Genius Party'] also like:\n",
            "1 Piper (2016)\n",
            "2 The Red Turtle (2016)\n",
            "3 Winnie the Pooh Goes Visiting (1971)\n",
            "4 A Plasticine Crow (1981)\n",
            "5 Cheburashka (1971)\n",
            "6 Travels of an Ant (1983)\n",
            "7 Wolf and Calf (1984)\n",
            "8 LEGO DC Super Hero Girls: Brain Drain (2017)\n",
            "9 Bunny (1998)\n",
            "10 Love Live! The School Idol Movie (2015)\n",
            "--------------Search based on similarity between user's preference--------------------------------------\n",
            "The users like ['Genius Party'] also like:\n",
            "1 Wallace & Gromit: The Best of Aardman Animation (1996)\n",
            "2 Dylan Moran: Monster (2004)\n",
            "3 Bill Hicks: Revelations (1993)\n",
            "4 Mulholland Dr. (1999)\n",
            "5 Eddie Izzard: Dress to Kill (1999)\n",
            "6 On the Beach (1959)\n",
            "7 Neon Genesis Evangelion: Death & Rebirth (Shin seiki Evangelion Gekijô-ban: Shito shinsei) (1997)\n",
            "8 Fist of Legend (Jing wu ying xiong) (1994)\n",
            "9 Grand Day Out with Wallace and Gromit, A (1989)\n",
            "10 Saving Face (2004)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7uRhT9Fxl49Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}